{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VD-CNN_Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Very Deep CNN for Text Classification Tasks\n",
        "\n",
        "Based on this paper: https://arxiv.org/pdf/1606.01781.pdf - Very Deep Convolutional Networks\n",
        "for Text Classification, Conneau et al.\n",
        "\n",
        "Following this implementation: https://github.com/cjiang2/VDCNN - have opened a pull request for an error in this implementation, commented it out in the relevant place in this file.\n",
        "\n",
        "The implementation above is broken into 3 parts: utility functions (Tokenizer), the VD-CNN code in TensorFlow, and a train loop. Wanted to code out the VD-CNN to get a good understanding of the architecture.\n",
        "\n",
        "Sometimes there is an issue with fetching the dataset from tensorflow datasets. Running Training Set-Up section again usually works it out.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Things to try:\n",
        "\n",
        "1. Paper suggests trying tasks with a lot more labels, possible to find other datasets? Movie genre classification possible?\n",
        "\n",
        "2. Implementing TPU\n",
        "\n",
        "3. Lot of hyperparameters / settings to tweak in this build.\n",
        "\n",
        "~Samyukt Sriram"
      ],
      "metadata": {
        "id": "bnCIKf28r1bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "ztfWmfKOA5ew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_D12WXjxrqe1"
      },
      "outputs": [],
      "source": [
        "#Tokenizer. This is in the utils.py file in the implementation.\n",
        "\n",
        "#Paper uses character based tokenizer, thought of as the atomic level of representation of text, similar to pixels for images.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Tokenizer(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        chars = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:’\"/|_#$%ˆ&*˜‘+=<>()[]{} ',\n",
        "        unk_token = True\n",
        "    ):\n",
        "\n",
        "        self.chars = chars\n",
        "        self.unk_token = 69 if unk_token == True else None\n",
        "        self.build()\n",
        "    \n",
        "    def build(self):\n",
        "        '''Build up char2idx'''\n",
        "        self.idx = 1 #Implementation mentions this is bc 0 is reserved for zero padding\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = {}\n",
        "\n",
        "        for char in self.chars:\n",
        "\n",
        "            #Each character has an ID (number) assigned\n",
        "            self.char2idx[char] = self.idx\n",
        "            self.idx2char[self.idx] = char\n",
        "            self.idx += 1\n",
        "    \n",
        "    def char_2_idx(self, c): #c is the character we want the ID for\n",
        "        '''returns the integer index ID for the character c'''\n",
        "\n",
        "        if not c in self.char2idx:\n",
        "            if self.unk_token is None:\n",
        "                return None\n",
        "            else: return self.unk_token\n",
        "        \n",
        "        return self.char2idx[c]\n",
        "    \n",
        "    def idx_2_chars(self, idx):\n",
        "        '''return the character for the index idx'''\n",
        "\n",
        "        #Unknown token case:\n",
        "        \n",
        "        if idx > len(self.idx2char):\n",
        "            if self.unk_token is None:\n",
        "                return ''\n",
        "            else: return '<UNK>'\n",
        "        \n",
        "        #Return empty string for 0 padding case\n",
        "\n",
        "        elif idx == 0:\n",
        "            return ''\n",
        "        \n",
        "        return self.idx2char[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        '''returns the length of the vocabulary'''\n",
        "        return len(self.char2idx)\n",
        "    \n",
        "    def text_to_sequence(self, text, maxlen = 1014): #paper specifies 1014, might be based on datasets and tasks. Possible to tweak?\n",
        "\n",
        "        text = text.lower() #Paper specifies this, might be interesting to try without and include caps in chars as well.\n",
        "        data = np.zeros(maxlen).astype(int)\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            if i > maxlen:\n",
        "                return data\n",
        "            if text[i] in self.char2idx:\n",
        "                data[i] = self.char_2_idx(text[i])\n",
        "        \n",
        "        return data\n",
        "    \n",
        "    def sequence_to_text(self, seq):\n",
        "        text = ''\n",
        "        for idx in seq:\n",
        "            text += self.idx_2_chars(idx)\n",
        "        \n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VD-CNN"
      ],
      "metadata": {
        "id": "xlY5vHblBA5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VD-CNN model\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "\n",
        "#This dictionary is only used in class VDCNN\n",
        "N_BLOCKS = {\n",
        "    9: (1,1,1,1),\n",
        "    17: (2,2,2,2),\n",
        "    29: (5,5,2,2),\n",
        "    49: (8,8,5,3)\n",
        "}\n",
        "\n",
        "class KMaxPooling(layers.Layer):\n",
        "\n",
        "  '''K-Max Pooling layer that extracts the k-highest activations from a sequence (2nd dimension)\n",
        "  TensorFlow Backend'''\n",
        "\n",
        "  #Read up more about what is going on here.\n",
        "\n",
        "  def __init__(self,\n",
        "               k=None,\n",
        "               sorted=False):\n",
        "    \n",
        "    super(KMaxPooling, self).__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.sorted = sorted\n",
        "\n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], self.k, input_shape[2])\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    if self.k is None:\n",
        "      k = int(tf.round(inputs.shape[1] / 2))\n",
        "    else: k = self.k\n",
        "\n",
        "    #Swapping the last 2 dimensions, bc top_k will be applied along the last dimension\n",
        "    #?\n",
        "    shifted_inputs = tf.transpose(inputs, [0,2,1])\n",
        "\n",
        "    #Extract top_k, returns 2 tensors = [values, indices]. \n",
        "    #Taking 0th element = values (?)\n",
        "    top_k = tf.nn.top_k(shifted_inputs, k=k, sorted= self.sorted)[0]\n",
        "\n",
        "    return tf.transpose(top_k, [0,2,1])\n",
        "\n",
        "\n",
        "class Pooling(layers.Layer):\n",
        "\n",
        "  '''Wrapper for different pooling operations. Included in maxpooling and k-maxpooling'''\n",
        "  #Again, read more about this and figure out what's going on here.\n",
        "  #https://www.youtube.com/watch?v=ZjM_XQa5s6s&ab_channel=deeplizard\n",
        "  #This helps extract features by taking max of sets of (pool_size). Also reduces computation\n",
        "\n",
        "  def __init__(self,\n",
        "               pool_type = 'max',\n",
        "               name = None):\n",
        "    super(Pooling, self).__init__(name=name)\n",
        "\n",
        "    assert pool_type in ['max', 'k_max']\n",
        "\n",
        "    self.pool_type = pool_type\n",
        "\n",
        "    if pool_type == 'max':\n",
        "      self.pool = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')\n",
        "    elif pool_type == 'k_max':\n",
        "      self.pool = KMaxPooling() #We defined this above\n",
        "    \n",
        "  def call(self, x):\n",
        "    return self.pool(x)\n",
        "\n",
        "class ZeroPadding(layers.Layer):\n",
        "\n",
        "  #https://www.youtube.com/watch?v=qSTv_m-KFk0&ab_channel=deeplizard\n",
        "  #This basically adds 0s around the input to preserve the size of the output after convolution\n",
        "\n",
        "  def __init__(self,\n",
        "               values,\n",
        "               name=None):\n",
        "    super(ZeroPadding, self).__init__(name=name)\n",
        "    self.values = values\n",
        "\n",
        "  def call(self,\n",
        "           x):\n",
        "    x = tf.pad(x, [[0,0], [0,0], [self.values[0],self.values[1]]], mode = 'CONSTANT', constant_values= 0)\n",
        "    return x\n",
        "\n",
        "class Conv1D_BN(layers.Layer):\n",
        "  '''A stack of conv 1x1 and BatchNorm'''\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size = 3,\n",
        "               strides = 2,\n",
        "               padding = 'same',\n",
        "               use_bias = True,\n",
        "               name=None):\n",
        "    super(Conv1D_BN, self).__init__(name=name)\n",
        "    self.filters = filters\n",
        "    self.use_bias = use_bias\n",
        "    self.conv = layers.Conv1D(filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias,\n",
        "                              kernel_initializer = 'he_normal')\n",
        "    self.bn = layers.BatchNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    x = self.conv(x)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class ConvBlock(layers.Layer):\n",
        "  '''Conv block with downsampling. 1x1 conv to increase dimensions'''\n",
        "  #What is downsampling, what does it mean to increase dimension and why\n",
        "  def __init__(\n",
        "      self,\n",
        "      filters,\n",
        "      kernel_size=3,\n",
        "      use_bias=True,\n",
        "      shortcut=True,\n",
        "      pool_type=None,\n",
        "      proj_type=None,\n",
        "      name=None,\n",
        "  ):\n",
        "    super(ConvBlock, self).__init__(name=name),\n",
        "    self.filters = filters\n",
        "    self.kernel_size = kernel_size\n",
        "    self.use_bias = use_bias\n",
        "    self.shortcut = shortcut\n",
        "    self.pool_type = pool_type\n",
        "    self.proj_type = proj_type\n",
        "\n",
        "    #dealing with downsampling and pooling\n",
        "    assert pool_type in ['max', 'k_max', 'conv', None]\n",
        "\n",
        "    if pool_type is None:\n",
        "      strides = 1\n",
        "      self.pool = None\n",
        "      self.downsample = None\n",
        "\n",
        "    elif pool_type == 'conv':\n",
        "      strides = 2 #Conv pool with stride = 2\n",
        "      #Note that the strides variable defined above is only used later, not in the self.downsample below.\n",
        "      self.pool = None\n",
        "      if shortcut:\n",
        "        self.downsample = Conv1D_BN(filters, 3, strides = 2, padding = 'same', use_bias = use_bias)\n",
        "    \n",
        "    else:\n",
        "      strides = 1\n",
        "      self.pool = Pooling(pool_type)\n",
        "      if shortcut:\n",
        "        self.downsample = Conv1D_BN(filters, 3, strides = 2, padding = 'same', use_bias = use_bias)\n",
        "    \n",
        "    #Defining layers\n",
        "    self.conv1 = layers.Conv1D(filters, kernel_size, strides = strides, padding='same', use_bias=use_bias, kernel_initializer='he_normal')\n",
        "    self.bn1 = layers.BatchNormalization()\n",
        "    self.conv2 = layers.Conv1D(filters, kernel_size, strides = 1, padding='same', use_bias=use_bias, kernel_initializer='he_normal')\n",
        "    self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "    assert proj_type in ['identity', 'conv', None]\n",
        "\n",
        "    if shortcut:\n",
        "      if proj_type == 'conv':\n",
        "      #1x1 conv, for projection\n",
        "        self.proj = Conv1D_BN(filters *2, 1, strides = 1, padding='same', use_bias=use_bias)\n",
        "      \n",
        "      elif proj_type == 'identity':\n",
        "        #Identity using 0 padding\n",
        "        self.proj = ZeroPadding([int(filters//2), filters - int(filters//2)])\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    residual = x #Used for skip connections if needed later\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = tf.nn.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.pool is not None:\n",
        "      out = self.pool(out)\n",
        "    \n",
        "    if self.shortcut:\n",
        "      if self.downsample is not None:\n",
        "        residual = self.downsample(residual)\n",
        "      out += residual\n",
        "    \n",
        "    out = tf.nn.relu(out)\n",
        "\n",
        "    if self.proj_type is not None and self.shortcut:\n",
        "      out = self.proj(out)\n",
        "    \n",
        "    return out\n",
        "\n",
        "class VDCNN(Model):\n",
        "  '''\n",
        "  Args:\n",
        "    num_classes: Number of classes for the classification task\n",
        "    depth: depth of the VDCNN - must be one of [9,17,29,49]\n",
        "    vocab_size: length of the vocabulary\n",
        "    seqlen: sequence length\n",
        "    embed_dim: dimension for character embedding\n",
        "    shortcut: Boolean, use skip connections\n",
        "    pool_type: Pooling operations, must be one of ['max', 'k_max', 'conv']\n",
        "    proj_type: Operation to increase dim for dotted skip connections, one of ['identity', 'conv']\n",
        "    use_bias: Use bias for all layers or not\n",
        "    logits: If False, returns softmax probabilities.\n",
        "  '''\n",
        "\n",
        "  def __init__(self,\n",
        "               num_classes,\n",
        "               depth=9,\n",
        "               vocab_size=69,\n",
        "               seqlen=None,\n",
        "               embed_dim=16,\n",
        "               shortcut=True,\n",
        "               pool_type='max',\n",
        "               proj_type = 'conv',\n",
        "               use_bias = True,\n",
        "               logits = True\n",
        "               ):\n",
        "    \n",
        "    super(VDCNN, self).__init__()\n",
        "    \n",
        "    self.num_classes = num_classes\n",
        "    self.depth = depth\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seqlen = seqlen\n",
        "    self.embed_dim = embed_dim\n",
        "    self.shortcut = shortcut\n",
        "    self.pool_type = pool_type\n",
        "    self.proj_type = proj_type\n",
        "    self.use_bias = use_bias\n",
        "    self.logits = logits #Error in original implementation, has this as = True when self.logits should be assigned = logits\n",
        "\n",
        "    assert pool_type in ['max', 'k_max', 'conv']\n",
        "    assert proj_type in ['conv', 'identity']\n",
        "\n",
        "    self.n_blocks = N_BLOCKS[depth]\n",
        "\n",
        "    self.embed_char = layers.Embedding(vocab_size, embed_dim, input_length=seqlen)\n",
        "    self.conv = layers.Conv1D(64, 3, strides=1, padding='same', use_bias=use_bias, kernel_initializer='he_normal')\n",
        "\n",
        "    \n",
        "    #In each block, only the last block has pooling and projection. that's why the for loop is -1\n",
        "    \n",
        "    #Convolutional Block 64\n",
        "    self.conv_block_64 = []\n",
        "\n",
        "    for _ in range(self.n_blocks[0] - 1):\n",
        "      self.conv_block_64.append(ConvBlock(64,3,use_bias, shortcut))\n",
        "    self.conv_block_64.append(ConvBlock(64,3,use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
        "\n",
        "    #Convolutional Block 128\n",
        "    self.conv_block_128 = []\n",
        "    \n",
        "    for _ in range(self.n_blocks[1] - 1):\n",
        "      self.conv_block_128.append(ConvBlock(128,3,use_bias, shortcut))\n",
        "    self.conv_block_128.append(ConvBlock(128,3,use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
        "\n",
        "    #Convolutional Block 256\n",
        "    self.conv_block_256 = []\n",
        "    \n",
        "    for _ in range(self.n_blocks[2] - 1):\n",
        "      self.conv_block_256.append(ConvBlock(256,3,use_bias, shortcut))\n",
        "    self.conv_block_256.append(ConvBlock(256,3,use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
        "\n",
        "    #Convolutional Block 512\n",
        "    self.conv_block_512 = []\n",
        "    \n",
        "    for _ in range(self.n_blocks[3] - 1):\n",
        "      self.conv_block_512.append(ConvBlock(512,3,use_bias, shortcut))\n",
        "    self.conv_block_512.append(ConvBlock(512,3,use_bias, shortcut, pool_type=pool_type, proj_type=proj_type))\n",
        "\n",
        "    self.k_maxpool = KMaxPooling(k=8) #Why is this a thing, turns out we use one final k_maxpooling operation after all these layers in call()\n",
        "   #But why is this 8?\n",
        "\n",
        "    self.flatten = layers.Flatten()\n",
        "\n",
        "    #Dense layers\n",
        "    self.fc1 = layers.Dense(2048, activation='relu')\n",
        "    self.fc2 = layers.Dense(2048, activation='relu')\n",
        "\n",
        "    self.out = layers.Dense(num_classes)\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    x = self.embed_char(x)\n",
        "    x = self.conv(x)\n",
        "\n",
        "    for l in self.conv_block_64:\n",
        "      x = l(x)\n",
        "    \n",
        "    for l in self.conv_block_128:\n",
        "      x = l(x)\n",
        "\n",
        "    for l in self.conv_block_256:\n",
        "      x = l(x)\n",
        "    \n",
        "    for l in self.conv_block_512:\n",
        "      x = l(x)\n",
        "\n",
        "\n",
        "    x = self.k_maxpool(x)\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    out = self.out(x)\n",
        "\n",
        "    if self.logits:\n",
        "      return out\n",
        "    else: return tf.nn.softmax(out)"
      ],
      "metadata": {
        "id": "dcKnXhzL7B_O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small test for the Model"
      ],
      "metadata": {
        "id": "MwcReABDBMk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing VDCNN\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  x = tf.zeros([4, 1014])\n",
        "\n",
        "  model = VDCNN(10, depth=9, shortcut=True, pool_type='max', proj_type='identity', logits = False)\n",
        "  out = model(x)\n",
        "  model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoQoUxt42-HD",
        "outputId": "c254420d-6cb3-44f8-88cb-e39eb2d6cf05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vdcnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  1104      \n",
            "                                                                 \n",
            " conv1d (Conv1D)             multiple                  3136      \n",
            "                                                                 \n",
            " conv_block (ConvBlock)      multiple                  37824     \n",
            "                                                                 \n",
            " conv_block_1 (ConvBlock)    multiple                  149376    \n",
            "                                                                 \n",
            " conv_block_2 (ConvBlock)    multiple                  593664    \n",
            "                                                                 \n",
            " conv_block_3 (ConvBlock)    multiple                  2366976   \n",
            "                                                                 \n",
            " k_max_pooling (KMaxPooling)  multiple                 0         \n",
            "                                                                 \n",
            " flatten (Flatten)           multiple                  0         \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  16779264  \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  4196352   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,148,186\n",
            "Trainable params: 24,142,426\n",
            "Non-trainable params: 5,760\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Set-Up"
      ],
      "metadata": {
        "id": "sQ0BsZzxBS5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#Hyperparameters\n",
        "\n",
        "MAXLEN = 1014\n",
        "DEPTH = 9\n",
        "EMBED_DIM = 16\n",
        "SHORTCUT = True\n",
        "POOL_TYPE = 'k_max'\n",
        "PROJ_TYPE = 'identity'\n",
        "USE_BIAS = True\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "SHUFFLE_BUFFER = 1024\n",
        "LR = 1e-2\n",
        "EPOCHS = 5\n",
        "CLIP_NORM = 7.0\n",
        "\n",
        "DATASET_NAME = 'ag_news'\n",
        "\n",
        "CHECKPOINT_PATH = './checkpoints'\n",
        "DISPLAY_EVERY = 20\n",
        "\n",
        "\n",
        "#Helper Functions\n",
        "#Some functions and objects used in these functions are only defined later. Might be a good idea to reorder this\n",
        "\n",
        "\n",
        "def prepare_data(dataset_name = 'ag_news', split='train'):\n",
        "\n",
        "  shuffle_files = True if split == 'train' else False\n",
        "\n",
        "  if dataset_name == 'ag_news':\n",
        "    ds = tfds.load('ag_news_subset', split=split, shuffle_files=shuffle_files)\n",
        "    num_classes = 4\n",
        "  \n",
        "  return ds, num_classes\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "\n",
        "  #Forward Pass\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(inputs, training=True)\n",
        "    loss = loss_object(labels, logits)\n",
        "  \n",
        "  #Backward Pass\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  if CLIP_NORM is not None:\n",
        "    #Using gradient clipping to stabilize the training\n",
        "    gradients = [tf.clip_by_norm(grad, CLIP_NORM) for grad in gradients]\n",
        "  \n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  #Metrics\n",
        "  preds = tf.nn.softmax(logits)\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, preds) #training accuracy\n",
        "\n",
        "@tf.function\n",
        "def test_step(inputs, labels):\n",
        "\n",
        "  logits = model(inputs, training = False)\n",
        "  t_loss = loss_object(labels, logits)\n",
        "\n",
        "  preds = tf.nn.softmax(logits)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, preds)\n",
        "\n",
        "\n",
        "#Training Prep\n",
        "\n",
        "#Dataset\n",
        "\n",
        "ds_train, num_classes = prepare_data(DATASET_NAME, 'train')\n",
        "ds_train = ds_train.shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test, _ = prepare_data(DATASET_NAME, 'test')\n",
        "ds_test = ds_test.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#Model\n",
        "model = VDCNN(num_classes = num_classes,\n",
        "              depth = DEPTH,\n",
        "              vocab_size = 69,\n",
        "              seqlen = MAXLEN,\n",
        "              embed_dim=EMBED_DIM,\n",
        "              shortcut=SHORTCUT,\n",
        "              pool_type=POOL_TYPE,\n",
        "              proj_type=PROJ_TYPE,\n",
        "              use_bias=USE_BIAS)\n",
        "\n",
        "#Optimizer\n",
        "#Could experiment with a different algorithm\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=LR, momentum=0.0)\n",
        "\n",
        "#Loss and Metrics\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "\n",
        "#Checkpoint\n",
        "ckpt = tf.train.Checkpoint(model=model)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_PATH, max_to_keep = 5) #setting max to keep as 5, =None in implementation"
      ],
      "metadata": {
        "id": "Bba_imVyFySI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Just to examine an example of the data\n",
        "\n",
        "for batch in ds_train:\n",
        "  print(batch['description'].numpy()[0])\n",
        "  print(tf.keras.utils.to_categorical(batch['label'][0], num_classes = num_classes))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZVd1XyNBkFN",
        "outputId": "b7819592-6814-45dc-ba3d-8b6beede1b22"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'FRX ECO INDUSAFIN ENG  2004-10-13 22:22:02 Fed #39;s McTeer tapped to head Texas A amp;M - UPDATE 1 WASHINGTON (AFX) -- Robert McTeer, president of the Federal Reserve Bank of Dallas, said he would retire from his post if named, as '\n",
            "[0. 0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "ZwnDGAhuBXZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Loop\n",
        "\n",
        "#Loop\n",
        "step = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  train_accuracy.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  #Training loop\n",
        "  for batch in ds_train:\n",
        "\n",
        "    #Modify this for different datasets?\n",
        "    texts = batch['description'].numpy() \n",
        "    labels = tf.keras.utils.to_categorical(batch['label'], num_classes = num_classes)\n",
        "\n",
        "    #Convert to sequence here\n",
        "    #Implementation mentions bypassing tfds in favour of a custom data operation. Unclear what this exactly means\n",
        "\n",
        "    inputs = np.array([tokenizer.text_to_sequence(text.decode('ascii')) for text in texts])\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    #One training step\n",
        "    train_step(inputs, labels)\n",
        "\n",
        "\n",
        "    #displaying progress\n",
        "    if step % DISPLAY_EVERY == 0:\n",
        "      print(f'Epoch: {epoch+1} \\n Step:{step} \\n Loss: {train_loss.result()} \\n Accuracy: {train_accuracy.result() * 100}')\n",
        "    \n",
        "    step += 1\n",
        "  \n",
        "  #Test Loop\n",
        "  \n",
        "  for batch_test in ds_test:\n",
        "\n",
        "    #Again, edit this for different datasets\n",
        "    texts = batch_test['description'].numpy()\n",
        "    labels = tf.keras.utils.to_categorical(batch_test['label'], num_classes=num_classes)\n",
        "\n",
        "    #converting to sequence\n",
        "    #again, mentions bypassing tfds in favour of a custom data operation. Unclear what this exactly means\n",
        "\n",
        "    inputs = np.array([tokenizer.text_to_sequence(text.decode('ascii')) for text in texts])\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    #1 test step\n",
        "    test_step(inputs, labels)\n",
        "\n",
        "  print(f'Epoch: {epoch+1} \\n Test Loss: {test_loss.result()} \\n Test Accuracy: {test_accuracy.result() * 100}')\n",
        "\n",
        "  #Saving model\n",
        "  ckpt_manager.save()"
      ],
      "metadata": {
        "id": "bb2ZEay1TMrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}