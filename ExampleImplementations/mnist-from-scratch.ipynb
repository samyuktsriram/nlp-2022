{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Coding out the MNIST digit recognition neural network using only numpy. From this youtube video: https://www.youtube.com/watch?v=w8yWXqWQ%20YmU&ab_channel=SamsonZhang\n\nJust as a way to practice and fully understand what's happening with basic neural network architecture.\n\n~Samyukt Sriram","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:39:53.204277Z","iopub.execute_input":"2022-06-02T14:39:53.204704Z","iopub.status.idle":"2022-06-02T14:39:53.209114Z","shell.execute_reply.started":"2022-06-02T14:39:53.204659Z","shell.execute_reply":"2022-06-02T14:39:53.208163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading in the data, manually doing train test split.\n\ndata = pd.read_csv('../input/digit-recognizer/train.csv')\ndata = np.array(data)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:39:53.248574Z","iopub.execute_input":"2022-06-02T14:39:53.249611Z","iopub.status.idle":"2022-06-02T14:39:56.102336Z","shell.execute_reply.started":"2022-06-02T14:39:53.249573Z","shell.execute_reply":"2022-06-02T14:39:56.101487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m , n = data.shape #n is number of features + 1, the label is present in first row\nnp.random.shuffle(data)\n\ndata_dev = data[0:1000].T #Transposed so each example is a column. Not clear on why this is important. Easier for indexing maybe?\nY_dev = data_dev[0]\nX_dev = data_dev[1:n]\nX_dev = X_dev / 255\n\ndata_train = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n]\nX_train = X_train / 255","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:39:56.103917Z","iopub.execute_input":"2022-06-02T14:39:56.105014Z","iopub.status.idle":"2022-06-02T14:39:56.758084Z","shell.execute_reply.started":"2022-06-02T14:39:56.104967Z","shell.execute_reply":"2022-06-02T14:39:56.757117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_params():\n    \n    W1 = np.random.rand(10, 784) - 0.5\n    b1 = np.random.rand(10,1) - 0.5\n    W2 = np.random.rand(10, 10) - 0.5\n    b2 = np.random.rand(10,1) - 0.5\n    \n    print(W1, b1, W2, b2)\n    return W1, b1, W2, b2\n\ndef ReLU(Z):\n    return np.maximum(Z, 0)\n\ndef softmax(Z):\n    a = np.exp(Z) / sum(np.exp(Z))\n    return a\n\ndef one_hot(Y):\n    \n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\ndef deriv_ReLU(Z):\n    return Z > 0\n\ndef forward_prop(W1, b1, W2, b2, X): #X is input\n    \n    Z1 = W1.dot(X) + b1\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = softmax(Z2)\n    \n    return Z1, A1, Z2, A2\n\ndef backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    \n    m = Y.size\n    one_hot_Y = one_hot(Y)\n    \n    dZ2 = A2 - one_hot_Y #Is this Cost?\n    \n    dW2 = 1 / m * dZ2.dot(A1.T)\n    db2 = 1 / m * np.sum(dZ2)\n    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    db1 = 1 / m * np.sum(dZ1)\n    \n    return dW1, db1, dW2, db2\n\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    \n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1\n    W2 = W2 - alpha * dW2\n    b2 = b2 - alpha * db2\n    \n    return W1, b1, W2, b2\n\ndef get_predictions(A2):\n    return np.argmax(A2, 0) #np.argmax returns an index, but we happen to want 0 to 9 anyway so this works out\n\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, iterations, alpha):\n    \n    #Init params\n    #for loop over iterations: forward_prop -> back_prop -> update params\n    #save params.\n    \n    W1, b1, W2, b2 = init_params()\n    for i in range(iterations):\n\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n\n        if i % 50 ==0:\n            print(f'Iteration: {i} \\n Accuracy:{get_accuracy(get_predictions(A2), Y)}')\n    return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:45:51.087271Z","iopub.execute_input":"2022-06-02T14:45:51.087999Z","iopub.status.idle":"2022-06-02T14:45:51.122958Z","shell.execute_reply.started":"2022-06-02T14:45:51.087951Z","shell.execute_reply":"2022-06-02T14:45:51.121766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 400, 0.1)\n\nprint(W1, b1, W2, b2)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:45:51.124694Z","iopub.execute_input":"2022-06-02T14:45:51.125976Z","iopub.status.idle":"2022-06-02T14:46:47.229057Z","shell.execute_reply.started":"2022-06-02T14:45:51.125916Z","shell.execute_reply":"2022-06-02T14:46:47.228102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}